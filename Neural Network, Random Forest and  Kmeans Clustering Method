#import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score,precision_score,recall_score , classification_report
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
root = "/content/DataSet_1.csv"
#Reading th CSV file
df = pd.read_csv(root)
df.info()
#We define a function to convert the objects into integers
def str_to_int(data):
  for col in data.columns:
    for i,item in enumerate(data[col]):
      if isinstance(item,str):
        try:
          data.at[i,col] = int(item)
          data.at[i,col] = float(item)
        except ValueError:
          if data[col].dtype == "object":
            labels, _= pd.factorize(data[col])
            data[col] = labels

  return data
str_to_int(df)
#The last column (target) is the output
X_1 = df.drop(["index","Patient Id","Level"],axis=1)
y_1 = df["Level"]
X_1
#Split test and train
X_train,X_test,y_train,y_test=train_test_split(X_1,y_1,test_size = 0.20, random_state = 42)
X_train.shape,X_test.shape,y_train.shape,y_test.shape
### Random Forest method
random_forest_clf = RandomForestClassifier(criterion='gini',n_estimators=20, max_depth=3, random_state=42)
random_forest_clf.fit(X_train,y_train)
y_pred = random_forest_clf.predict(X_test)
print(classification_report(y_test,y_pred))
accuracy_random_forest = accuracy_score(y_test,y_pred)
print(f"The accuracy for random forest is {accuracy_random_forest}")
importances = random_forest_clf.feature_importances_
print(f"The importance for random forest is {importances}")
estimator = random_forest_clf.estimators_[0]
fig,ax = plt.subplots(figsize = (10,6))
plot_tree(estimator, feature_names=df.columns, filled=True, ax=ax)
plt.show()

importances = random_forest_clf.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.bar(range(X_1.shape[1]), importances[indices], align='center')
plt.xticks(range(X_1.shape[1]), [df.columns[i] for i in indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances')
plt.show()
n_trees = len(random_forest_clf.estimators_)

# Plot each tree
fig, axes = plt.subplots(nrows=1, ncols=n_trees, figsize=(20, 4), dpi=300)

for idx, estimator in enumerate(random_forest_clf.estimators_):
    ax = axes[idx]
    plot_tree(estimator, feature_names=df.columns, filled=True, ax=ax)

plt.tight_layout()
plt.show()
## Kmeans clustering method
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import math
root = "/content/heart.csv"
#Reading th CSV file
data = pd.read_csv(root)
data_1 = data.drop(["cl","aq"],axis=1)
data_2=data_1.dropna()
data_2
data_2.info()
#The last column (target) is the output
X_2 =data_2.drop(["target"],axis=1)
y_2 =data_2["target"]
X_2
y_2

X_2=data_2.iloc[:, :-1].values
y_2=data_2.iloc[:,-1].values
kmeans = KMeans(n_clusters = 3, random_state = 42)
kmeans.fit(X_2)
# Visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X_2[:, 0], X_2[:, 4], c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 4], marker='x', c='r', s=150)
plt.title('K-Means clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
# Elbow method for choosing K
inertias = []
for k in range(1, 20):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(1, 20), inertias, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Choosing K')
plt.show()
y_pred = kmeans.labels_
accuracy = accuracy_score(y, y_pred)
print(f"Accuracy: {accuracy:.2f}")
## Neural Network
from sklearn.neural_network import MLPClassifier
import networkx as nx
#Split test and train
X_train,X_test,y_train,y_test=train_test_split(X_2,y_2,test_size = 0.20, random_state = 42)
mlp = MLPClassifier(activation='logistic',max_iter=10000, random_state=42,hidden_layer_sizes=(20,10,5,3))
mlp.fit(X_train, y_train)
# Predict and calculate accuracy
y_pred = mlp.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
# Function to plot the MLPClassifier structure
def plot_mlp_structure(mlp, input_size):
    layers = [input_size] + list(mlp.hidden_layer_sizes) + [1]  # Add input and output layers

    G = nx.DiGraph()
    node_count = 0
    layer_nodes = []

    # Create nodes for each layer
    for i, layer_size in enumerate(layers):
        layer_nodes.append([])
        for _ in range(layer_size):
            G.add_node(node_count, layer=i)
            layer_nodes[-1].append(node_count)
            node_count += 1

    # Create edges between nodes of subsequent layers
    for i in range(len(layer_nodes) - 1):
        for node in layer_nodes[i]:
            for next_node in layer_nodes[i + 1]:
                G.add_edge(node, next_node)

    pos = {}
    for i, layer in enumerate(layer_nodes):
        for j, node in enumerate(layer):
            pos[node] = (i, j - len(layer) / 2)

    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=False, node_size=500, node_color="skyblue", edge_color="gray")
    labels = {i: f"Input {i+1}" for i in range(input_size)}
    hidden_layers = sum([[f"H{j+1}" for j in range(size)] for size in mlp.hidden_layer_sizes], [])
    labels.update({i + input_size: hidden_layers[i] for i in range(len(hidden_layers))})
    labels.update({node_count - 1: "Output"})
    nx.draw_networkx_labels(G, pos, labels, font_size=10)
    plt.title("MLPClassifier Structure")
    plt.show()


# Plot the MLPClassifier structure
plot_mlp_structure(mlp, X.shape[1])
